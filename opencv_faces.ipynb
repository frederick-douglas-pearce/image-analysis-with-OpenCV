{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df89f473-caf3-4850-bcf2-e371a0cc64d8",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition\n",
    "This notebook provides a quick resource for exploring OpenCV's built-in face detection and recognition algorithms, and is inspired by the third section of the following youtube video: https://www.youtube.com/watch?v=oXlwWbU8l2o\n",
    "\n",
    "Original code for the course can be found on the [github profile](https://github.com/jasmcaus/opencv-course) of the course instructor, Jason Dsouza.\n",
    "\n",
    "Images of 'famous' people are used as examples, which were downloaded from the Labelled Faces in the Wild (LFW) dataset, available as part of the following Kaggle competition: https://www.kaggle.com/jessicali9530/lfw-dataset\n",
    "\n",
    "The goal of Face Detection is to identify a rectangular bounding box around each face in an input image. Additional features may be detected as well, e.g. eyes, smile, etc. The goal of Face Recognition is to correctly identify the person based on their facial features. Common practice is to first apply face detection, then apply a facial recognition algorithm to identify the name of the person from features in the detected face image.\n",
    "\n",
    "## References\n",
    "- https://github.com/Kaggle/kaggle-api (you can use the api to download the LFW data)\n",
    "- https://pythonprogramming.net/haar-cascade-object-detection-python-opencv-tutorial/\n",
    "- https://towardsdatascience.com/a-dog-detector-and-breed-classifier-4feb99e1f852 (also see https://github.com/HenryDashwood/dog_breed_classifier)\n",
    "- https://keras.io/guides/transfer_learning/\n",
    "- https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81\n",
    "- https://www.pyimagesearch.com/2014/07/21/detecting-circles-images-using-opencv-hough-circles/\n",
    "- https://stackoverflow.com/questions/20801015/recommended-values-for-opencv-detectmultiscale-parameters\n",
    "- https://docs.opencv.org/4.1.0/d5/d54/group__objdetect.html\n",
    "- https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a801c3-a78e-4979-bc9e-a51af8990770",
   "metadata": {},
   "source": [
    "### 0. Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08811096-1863-435b-8278-ae2a74f65167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from opencv_tools import load_frame_gray, show_frame\n",
    "from opencv_tools import detect_primary_objects, detect_all_objects, detect_image_objects, draw_detected_objects, get_detected_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf251e-ad77-48a9-8a40-916cb50e2c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Set Image Data Parameters\n",
    "The following parameters are used to load the image data:\n",
    "\n",
    "`dir_root` = A string defining the base directory that contains subfolders with image files for each person\n",
    "\n",
    "`people` = A list containing strings that are the names of the \"famous\" people, and that are the names of the subfolders containing image data. Each name must match the subfolder name exactly after a single modification: spaces, \" \", are converted to underscores, \"_\" (see get_img_path for details). The Analysis in Section 2 provides a list of the people with the most image data that can be used to populate this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5777731f-42d8-4167-bc2c-c3c3774d2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = \"/home/fdpearce/Documents/Projects/data/Images/LFW/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "people = [\"George W Bush\", \"Laura Bush\", \"Vladimir Putin\", \"Gloria Macapagal Arroyo\", \"Arnold Schwarzenegger\", \"Megawati Sukarnoputri\", \\\n",
    "          \"Hugo Chavez\", \"Serena Williams\", \"Colin Powell\", \"Junichiro Koizumi\", \"Jennifer Capriati\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709138c8-9438-4219-a879-5cdbe93bcf13",
   "metadata": {},
   "source": [
    "### 2. Image Availablility Analysis\n",
    "Print high-level stats on the amount of image data available in the dataset, including \n",
    "  1. the number of people with image data\n",
    "  2. a list of people that have the most images, along with their image count. `num_people_with_most_img` determines how many people to include in the output list. The following parameter is required to perform the analysis:\n",
    "\n",
    "`num_people_with_most_img` = an integer specifying the number of people to list when displaying the people with the most image data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecddecb-ec35-4ee5-842e-8e00d8fb1df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of People with Images Available = 5749\n",
      "Top 25 People With Most Images, # of Images\n",
      "('George_W_Bush', 530)\n",
      "('Colin_Powell', 236)\n",
      "('Tony_Blair', 144)\n",
      "('Donald_Rumsfeld', 121)\n",
      "('Gerhard_Schroeder', 109)\n",
      "('Ariel_Sharon', 77)\n",
      "('Hugo_Chavez', 71)\n",
      "('Junichiro_Koizumi', 60)\n",
      "('Jean_Chretien', 55)\n",
      "('John_Ashcroft', 53)\n",
      "('Serena_Williams', 52)\n",
      "('Jacques_Chirac', 52)\n",
      "('Vladimir_Putin', 49)\n",
      "('Luiz_Inacio_Lula_da_Silva', 48)\n",
      "('Gloria_Macapagal_Arroyo', 44)\n",
      "('Arnold_Schwarzenegger', 42)\n",
      "('Jennifer_Capriati', 42)\n",
      "('Laura_Bush', 41)\n",
      "('Lleyton_Hewitt', 41)\n",
      "('Hans_Blix', 39)\n",
      "('Alejandro_Toledo', 39)\n",
      "('Nestor_Kirchner', 37)\n",
      "('Andre_Agassi', 36)\n",
      "('Alvaro_Uribe', 35)\n",
      "('Megawati_Sukarnoputri', 33)\n"
     ]
    }
   ],
   "source": [
    "num_people_with_most_img = 25\n",
    "# Params above, code below\n",
    "people_folders = os.listdir(dir_root)\n",
    "num_images_per_folder = [len(os.listdir(os.path.join(dir_root, p))) for p in people_folders]\n",
    "sort_people_num_img = sorted(zip(people_folders, num_images_per_folder), reverse=True, key=lambda pair: pair[1])\n",
    "print(f\"# of People with Images Available = {len(people_folders)}\")\n",
    "print(f\"Top {num_people_with_most_img} People With Most Images, # of Images\")\n",
    "print(*sort_people_num_img[:num_people_with_most_img], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68791b3e-533f-4d35-93ed-e1be08a9f1de",
   "metadata": {},
   "source": [
    "### 3. Define Functions\n",
    "These project-specific functions are used for creating training data and visualizing/validating steps in the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabf2c55-feff-4cba-a4e9-aa9f68ad2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_path(dir_root, person, img_num, img_ext):\n",
    "    \"\"\"Returns the full path to an image by joining the parameters in their input order:\n",
    "    dir_root = directory root, one level above folders for individual people that contain image files\n",
    "    person = person the images are of in a specific subfolder, with spaces between first, last, etc\n",
    "    img_num = a string identifying the individual image of the person\n",
    "    img_ext = file extension of the image files\n",
    "    Image format is roughly dir_root/person/person+_+img_num+_+img_ext, see code for details\n",
    "    \"\"\"\n",
    "    person_fname = \"_\".join(person.split(\" \"))\n",
    "    file_name = \"_\".join([person_fname, img_num])+img_ext\n",
    "    return os.path.join(dir_root, person_fname, file_name)\n",
    "\n",
    "def create_training_data(dir_root, people, person_img_max, detect_params, detect_objects=False, detect_type=\"all\", random_seed=0, verbose=False):\n",
    "    \"\"\"Create training data from the following inputs:\n",
    "    dir_root = a string containing base directory with subfolders containing images for each person\n",
    "    people = a list containing string values, with each string containing a person's name. These are the folder names, EXCEPT for spaces instead of underscores\n",
    "    person_img_max = an integer specifying the maximum number of training images to return for each person. Set this to a value larger than that\n",
    "                     maximum # of images available if you want to process all images\n",
    "    haar_path = a string containing the path to the haar cascade class definition xml file. Files for different features (e.g. face, eyes, etc) can\n",
    "                be downloaded from the following link: https://github.com/opencv/opencv/blob/master/data/haarcascades/\n",
    "    detect_params = a dictionary containing two sets of parameters: 1) 'haar_file', a string specifying the full path to the haar cascade\n",
    "                    xml file to load and 2) 'params' dict to pass to the detectMultiScale method of the haar cascade class. Valid values\n",
    "                    include scaleFactor (default=1.1), minNeighbors (default=3), and minSize. \n",
    "                    The haar_file in 1) can be downloaded from here: https://github.com/opencv/opencv/blob/master/data/haarcascades/\n",
    "    detect_objects = a boolean-like value (True/False, 1/0, etc) that turns on/off object detection. Set to false to conduct a dry run that checks\n",
    "                     how many image files will be processed for each person\n",
    "    detect_type = an optional string specifying the type of detection to perform:\n",
    "                  \"all\": runs detect_all_objects, which returns all objects detected from one execution of the haar class detectMultiScale\n",
    "                  method with the input parameters specified in detect_params. The number of objects detected may vary greatly from image to\n",
    "                  image for a fixed set of input parameters\n",
    "                  \"primary\": runs detect_primary_objects, which performs an iterative process to return a user-specified number of primary objects\n",
    "                  detected in the input image. Essentially, the minNeighbors parameter is adjusted until the desired number of objects are detected\n",
    "    random_seed = an integer that determines the order of the random shuffle applied to the list of image files before selecting which to include as training samples\n",
    "    verbose = a boolean-like value that, when truthy, prints additional details during execution for validation/debugging purposes\n",
    "    Output is a tuple with two values:\n",
    "    features = a list containing zero or more lists, with each list containing four values (x, y, w, h) that define the rectangle containing the detected object\n",
    "    labels = a list containing zero or more integer values, with each int specifying the index to a specific person in the input list of people used for training\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for person in people:\n",
    "        label = people.index(person)\n",
    "        path = os.path.join(dir_root, \"_\".join(person.split(\" \")))\n",
    "        img_files = os.listdir(path)\n",
    "        random.shuffle(img_files)\n",
    "        img_num = 0\n",
    "        total_detected = 0\n",
    "        num_detected = 0\n",
    "        for img_f in img_files[:min(person_img_max, len(img_files))]:\n",
    "            img_path = os.path.join(path, img_f)\n",
    "            img_num += 1\n",
    "            print(f\"Working on Image # {img_num}: {img_f}\")\n",
    "            if detect_objects:\n",
    "                _, gray = load_frame_gray(img_path, gray_flag=True)\n",
    "                detected_features, detected_labels = detect_image_objects(gray, detect_params, detect_type=detect_type, label=label, verbose=verbose)\n",
    "                num_detected = len(detected_features)\n",
    "                if num_detected:\n",
    "                    total_detected += num_detected\n",
    "                    features.extend(detected_features)\n",
    "                    labels.extend(detected_labels)\n",
    "        print(f\"{img_num} training images with {total_detected} objects identified for {person}\")\n",
    "    return features, labels\n",
    "\n",
    "def show_person_images(dir_root, person, img_nums, img_ext, object_rectangles, rect_color=(0, 0, 0)):\n",
    "    \"\"\"Loop through each each image in the input person's subfolder, whose number is provided in the input list of\n",
    "    strings, img_nums. Show all objects in the input list of objects, object_rectangles.\n",
    "    \"\"\"\n",
    "    for ind, img_n in enumerate(img_nums):\n",
    "        img_path = get_img_path(dir_root, person, img_n, img_ext)\n",
    "        _, gray = load_frame_gray(img_path, gray_flag=True)\n",
    "        gray = draw_detected_objects(gray, object_rectangles[ind], print_detected=True, frame_to_show=None, rect_color=rect_color)\n",
    "        show_frame(gray, f\"Primary Objects(s) Detected for {person}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4620061-61e9-4f26-bbfe-43758cf660fa",
   "metadata": {},
   "source": [
    "### 4. Face Detection Example\n",
    "\n",
    "#### 4.1 Load Example Image File\n",
    "First specify which person you want to analyze from the list, `people`, defined above, by providing a valid index value. Then specify the number of the image (see image files for details), `img_num`, and finally provide the extension for the image file (e.g. \".jpg\", \".png\", etc), `img_ext`. The code will load the image into a variable, `gray_face`, for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c047bf-d25f-45b4-b98f-78a59e198cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Example Grayscale Image = (250, 250)\n"
     ]
    }
   ],
   "source": [
    "person = people[5]\n",
    "img_num = \"0027\"\n",
    "img_ext = \".jpg\"\n",
    "# Params above, code below\n",
    "img_path = get_img_path(dir_root, person, img_num, img_ext)\n",
    "_, gray_face = load_frame_gray(img_path, gray_flag=True)\n",
    "gray_face_orig = gray_face.copy()\n",
    "print(f\"Size of Example Grayscale Image = {gray_face.shape}\")\n",
    "show_frame(gray_face, f\"Example Image of {person}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a657b4-3001-4e6a-b14c-10bb2fe365cb",
   "metadata": {},
   "source": [
    "#### 4.2 Download Haar Cascade XML File for Face Detection\n",
    "The first step is to download Haar Cascades xml file for frontal face detection from the following link:\n",
    "\n",
    "https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "\n",
    "If you want to follow the code below, then you should also download the eye and smile detection xml files from the haarcascades folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec2a2d-7a61-4c17-b9a3-9fce1d364c66",
   "metadata": {},
   "source": [
    "#### 4.3 Classic Haar Cascade Detection\n",
    "##### **Define Detection Parameters**\n",
    "\n",
    "The dictionary below defines the parameters required to perform object detection (e.g. face, eye, smile) using Haar Cascades. The `haar_file` key defines the full path to the Haar Cascade XML file downloaded in step 4.2 above. The `params` dict can take any key/value pair that is a valid input to the detectMultiScale method of the haar cascade class. The default values provided below should work for most applications, but feel free to change them to optimize performance, etc. See [this link](https://docs.opencv.org/4.1.0/d1/de5/classcv_1_1CascadeClassifier.html) for additional details, such as the default values used in OpenCV for these parameters, additional parameters that can be included, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba68f3e-ef8b-47e3-a663-7a6e665448e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Haar Cascade detection parameters as necessary\n",
    "haar_cascade_params = {\n",
    "    'face': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_frontalface.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 4,\n",
    "            'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'eye': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_eye.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 4,\n",
    "            'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'smile': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_smile.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 8,\n",
    "            'minSize': (20, 20)\n",
    "        }     \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fcce5-4661-4411-a4c5-604c315238cf",
   "metadata": {},
   "source": [
    "##### **Detect All Faces, Eyes, and Smiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebad2d2b-4f12-4160-a8c3-a88cca6dfcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Face Objects Detected = 2\n",
      "# of Eye Objects Detected = 4\n",
      "# of Smile Objects Detected = 3\n"
     ]
    }
   ],
   "source": [
    "detection_params = {\n",
    "    'objects_to_detect': ['face', 'eye', 'smile'],\n",
    "    'detector_func': detect_all_objects,\n",
    "    'verbose': True\n",
    "}\n",
    "detected_rects = get_detected_objects(gray_face, haar_cascade_params, **detection_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04846c-f722-4b90-a5a9-3154de099f04",
   "metadata": {},
   "source": [
    "##### **Display Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb35bfa-51d4-46d7-af90-c56b39b4370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of Faces Found = 2\n",
      "Object 0 Location: x=7, y=49, w=102, h=102\n",
      "Object 1 Location: x=74, y=69, w=109, h=109\n",
      "\n",
      "# of Eyes Found = 4\n",
      "Object 0 Location: x=129, y=98, w=30, h=30\n",
      "Object 1 Location: x=95, y=100, w=26, h=26\n",
      "Object 2 Location: x=33, y=70, w=26, h=26\n",
      "Object 3 Location: x=71, y=138, w=26, h=26\n",
      "\n",
      "# of Smiles Found = 3\n",
      "Object 0 Location: x=121, y=195, w=46, h=23\n",
      "Object 1 Location: x=100, y=144, w=57, h=29\n",
      "Object 2 Location: x=33, y=85, w=208, h=104\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n# of Faces Found = {len(detected_rects['face'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['face'], print_detected=True, frame_to_show=None, rect_color=(0, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s) Detected in Image of {person}\")\n",
    "print(f\"\\n# of Eyes Found = {len(detected_rects['eye'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['eye'], print_detected=True, frame_to_show=None, rect_color=(200, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s)+Eye(s) Detected in Image of {person}\")\n",
    "print(f\"\\n# of Smiles Found = {len(detected_rects['smile'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['smile'], print_detected=True, frame_to_show=None, rect_color=(255, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s)+Eye(s)+Smile(s) Detected in Image of {person}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86cb1-2f80-4863-809e-ce8edad5263f",
   "metadata": {},
   "source": [
    "#### 4.4 Primary Haar Cascade Detection\n",
    "##### **Define Primary Detection Parameters**\n",
    "\n",
    "Note that Primary Detection only requires one additional parameter:\n",
    "\n",
    "`num_primary_obj` = the number of \"primary\" objects to detect within the image through adjustments to the minNeighbors parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31f3d40-64e2-499e-80be-c57ae07fcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Haar Cascade detection parameters as necessary\n",
    "haar_cascade_params = {\n",
    "    'face': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_frontalface.xml',\n",
    "        'num_primary_obj': 1,\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 4,\n",
    "            'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'eye': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_eye.xml',\n",
    "        'num_primary_obj': 2,\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 4,\n",
    "            'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'smile': {\n",
    "        'haar_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_smile.xml',\n",
    "        'num_primary_obj': 1,\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 8,\n",
    "            'minSize': (20, 20)\n",
    "        }     \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef488df3-772a-4313-acfd-619409fb4964",
   "metadata": {},
   "source": [
    "##### **Detect Primary Faces, Eyes, and Smiles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a54b36-5a7d-4272-a948-4f5c6f8e5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial # of Face Objects Detected = 2\n",
      "Iteration # = 1\n",
      "minNeighbors = 6\n",
      "Iteration # = 2\n",
      "minNeighbors = 10\n",
      "Iteration # = 3\n",
      "minNeighbors = 16\n",
      "Iteration # = 4\n",
      "minNeighbors = 24\n",
      "Final # of Face Objects Detected = 1\n",
      "\n",
      "Initial # of Eye Objects Detected = 4\n",
      "Iteration # = 1\n",
      "minNeighbors = 6\n",
      "Final # of Eye Objects Detected = 2\n",
      "\n",
      "Initial # of Smile Objects Detected = 3\n",
      "Iteration # = 1\n",
      "minNeighbors = 10\n",
      "Iteration # = 2\n",
      "minNeighbors = 12\n",
      "Iteration # = 3\n",
      "minNeighbors = 16\n",
      "Iteration # = 4\n",
      "minNeighbors = 22\n",
      "Final # of Smile Objects Detected = 1\n"
     ]
    }
   ],
   "source": [
    "gray_face = gray_face_orig\n",
    "detection_params = {\n",
    "    'objects_to_detect': ['face', 'eye', 'smile'],\n",
    "    'detector_func': detect_primary_objects,\n",
    "    'verbose': True\n",
    "}\n",
    "detected_rects = get_detected_objects(gray_face, haar_cascade_params, **detection_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77baf9-c873-4865-b87f-f85d623211f6",
   "metadata": {},
   "source": [
    "##### **Display Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e65103b-293f-43ab-8656-662b0fc0c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of Primary Faces Found = 1\n",
      "Object 0 Location: x=74, y=69, w=109, h=109\n",
      "\n",
      "# of Primary Eyes Found = 2\n",
      "Object 0 Location: x=129, y=98, w=30, h=30\n",
      "Object 1 Location: x=95, y=100, w=26, h=26\n",
      "\n",
      "# of Primary Smiles Found = 1\n",
      "Object 0 Location: x=100, y=144, w=57, h=29\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n# of Primary Faces Found = {len(detected_rects['face'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['face'], print_detected=True, frame_to_show=None, rect_color=(0, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s) Detected in Image of {person}\")\n",
    "print(f\"\\n# of Primary Eyes Found = {len(detected_rects['eye'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['eye'], print_detected=True, frame_to_show=None, rect_color=(200, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s)+Eye(s) Detected in Image of {person}\")\n",
    "print(f\"\\n# of Primary Smiles Found = {len(detected_rects['smile'])}\")\n",
    "gray_face = draw_detected_objects(gray_face, detected_rects['smile'], print_detected=True, frame_to_show=None, rect_color=(255, 0, 0))\n",
    "show_frame(gray_face, f\"Faces(s)+Eye(s)+Smile(s) Detected in Image of {person}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1054f3b-063d-41aa-9556-71d454d1210c",
   "metadata": {},
   "source": [
    "#### 4.6 Summary\n",
    "- **Face Detection**\n",
    "  - **Classic Detection**: Two faces are detected, and both faces are true positives, but that isn't the desired behavior, as the label only applies to one of the faces. This is a serious challenge when preprocessing the dataset to train a face recognizer: how do we eliminate \"secondary\" faces so our labels are correctly applied? Note that techniques meant to consolidate multiple detections of the same face (e.g. Non-Maximum Suppression) won't help, and it's likely a more expensive deep learning algorithm would pick these secondary faces up even more consistently than the Haar Cascades used here. For example secondary faces tend to be more rotated away from the camera, out of focus, etc, so Haar Cascades don't detect them as easily.\n",
    "  - **Primary Detection**: `detect_primary_objects` addresses the secondary faces problem (and false positives), detecting one, primary face within the input image using the same parameters input during Classic Detection in Section 4.3. As we'll see in Section 5, the dataset would contain many mislabeled, inacurrate faces if this issue wasn't resolved, leading to degredation of the facial recognition model's performance, a known issue discussed in the course.\n",
    "- **Eye Detection**\n",
    "  - **Classic Detection**: Four eyes are detected, which makes sense given the two faces; however, only three of the eyes are true positives, with the false positive erroneously identifying a curved hand shape as an eye. Overall, the eye detector works pretty well, and this is without limiting its input image to only the region of interest where a face was detected; however, you still can't constrain the output to conform to reason: each face should only have two eyes (or one if it's a profile view). \n",
    "  - **Primary Detection**: Only the two eyes on the primary person's face are returned when using `detect_primary_objects` with num_primary_obj = 2, even though the entire image was input, not just the part detected as a face. Impressive! It is worth considering eye detection as an addition data validation step in the training data pipeline. For example, each primary face that is detected must also contain two primary eyes within its bounds in order to be included in the training dataset.\n",
    "- **Smile Detection**\n",
    "  - **Classic Detection**: Three smiles were detected: one posible true positive (its definitely a mouth, but one can debate whether it's a smile...), but the other two detections are clearly bad false positives. Overall, the smile detector has pretty good recall but bad precision. Again, better results would be obtained using only the image detected around the faces as input, but even then, I've found smile detection to be the most error prone of the three object types, by far. \n",
    "  - **Primary Detection**: Only the correct smile is detected by `detect_primary_objects`, again, even when the whole image is input. As a general rule, primary smile detection requires quite a few iterations to reach a large value for minNeighbors, so the default value may need adjusting, but I've found it to work pretty well so far. It would be interesting to see whether there are patterns in the way the algorithm converges that could be used to identify true vs false positives, probably not, but who knows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c0e43-153e-490b-841b-767ce8887ab4",
   "metadata": {},
   "source": [
    "### Face Detection on Training Data\n",
    "First perform face detection using detection type of 'all', i.e. detection parameters are fixed to those provided at input and all detected objects are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a059c2-d9c2-497e-a298-6bd45a8de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_max = 30\n",
    "detect_obj = 'face'\n",
    "detect_type = 'all'\n",
    "# Params above, code below\n",
    "features, labels = create_training_data(dir_root, people, person_img_max, haar_cascade_params[detect_obj], detect_objects=True, detect_type=detect_type, verbose=True)\n",
    "print(f\"# of Features = {len(features)}\")\n",
    "print(f\"# of Labels = {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad592d-227f-432f-a312-b793610f87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect example images and the bounding box detected by the face recognition algorithm\n",
    "# Use the verbose setting above to print out bounding boxes\n",
    "person = people[0]\n",
    "img_nums = [\"0003\", \"0019\", \"0025\", \"0027\", \"0031\"]\n",
    "img_ext = \".jpg\"\n",
    "# Megawati Sukarnoputri: 'all' examples where 2 faces detected instead of one\n",
    "face_rectangles = [[[92, 46, 86, 86], [70, 77, 107, 107]], \\\n",
    "                   [[69, 69, 112, 112], [86, 161, 80, 80]], \\\n",
    "                   [[3, 10, 74, 74], [67, 68, 115, 115]], \\\n",
    "                   [[7, 49, 102, 102], [74, 69, 109, 109]], \\\n",
    "                   [[162, 8, 56, 56], [65, 68, 117, 117]]]\n",
    "show_person_images(dir_root, person, img_nums, img_ext, face_rectangles, rect_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1816058-18ce-4bf9-a343-5134746fa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform face detection using detection type of 'primary' for comparison\n",
    "detect_type = 'primary'\n",
    "haar_obj = haar_cascade_params[detect_obj]\n",
    "features, labels = create_training_data(dir_root, people, person_img_max, haar_obj, detect_objects=True, detect_type=detect_type, verbose=True)\n",
    "print(f\"# of Features = {len(features)}\")\n",
    "print(f\"# of Labels = {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e0ca9-d5c1-4d8b-9057-0bc35008253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_rectangles = [[[70, 77, 107, 107]], \\\n",
    "                   [[69, 69, 112, 112]], \\\n",
    "                   [[67, 68, 115, 115]], \\\n",
    "                   [[74, 69, 109, 109]], \\\n",
    "                   [[65, 68, 117, 117]]]\n",
    "show_person_images(dir_root, person, img_nums, img_ext, face_rectangles, rect_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a32bf7-6694-4455-9679-5e56cea19398",
   "metadata": {},
   "source": [
    "### 2. Train Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa0419-ab32-46ef-83ac-ccd1f325e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_recognizer_data = False\n",
    "face_recognizer = cv.face.LBPHFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954fac-95cf-4d7f-8220-bead53aea7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features, dtype=\"object\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fc96f-d9fc-4dd4-be7e-b881a835b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognizer.train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a1b1d-adb5-4d54-a99d-c2871fca2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_recognizer_data:\n",
    "    face_recognizer.save(\"face_recognizer.yml\")\n",
    "    np.save(\"features.npy\", features)\n",
    "    np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9407f1-9ea9-4a8f-a309-c74e494db7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
