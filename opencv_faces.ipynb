{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df89f473-caf3-4850-bcf2-e371a0cc64d8",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition\n",
    "This notebook provides a quick resource for exploring OpenCV's built-in face detection and recognition algorithms, based on the third section of the following youtube video: https://www.youtube.com/watch?v=oXlwWbU8l2o\n",
    "\n",
    "Images of 'famous' people are used as examples, which were downloaded from the Labelled Faces in the Wild (LFW) dataset, available as part of the following Kaggle competition: https://www.kaggle.com/jessicali9530/lfw-dataset\n",
    "\n",
    "The goal of Face Detection is to identify a rectangular bounding box around each face in an input image. Additional features may be detected as well, e.g. eyes, smile, etc. The goal of Face Recognition is to correctly identify the person based on their facial features. Common practice is to first apply face detection, then apply a facial recognition algorithm to identify the name of the person from features in the detected face image.\n",
    "\n",
    "## References\n",
    "- https://github.com/Kaggle/kaggle-api (you can use the api to download the data)\n",
    "- https://pythonprogramming.net/haar-cascade-object-detection-python-opencv-tutorial/\n",
    "- https://towardsdatascience.com/a-dog-detector-and-breed-classifier-4feb99e1f852 (also see https://github.com/HenryDashwood/dog_breed_classifier)\n",
    "- https://keras.io/guides/transfer_learning/\n",
    "- https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81\n",
    "- https://www.pyimagesearch.com/2014/07/21/detecting-circles-images-using-opencv-hough-circles/\n",
    "- https://stackoverflow.com/questions/20801015/recommended-values-for-opencv-detectmultiscale-parameters\n",
    "- https://docs.opencv.org/4.1.0/d5/d54/group__objdetect.html\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08811096-1863-435b-8278-ae2a74f65167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5777731f-42d8-4167-bc2c-c3c3774d2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = r\"/home/fdpearce/Documents/Projects/data/Images/LFW/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "# Name must match folder name exactly after a single modification: spaces, \" \", are converted to underscores, \"_\"\n",
    "people = [\"George W Bush\", \"Laura Bush\", \"Vladimir Putin\", \"Gloria Macapagal Arroyo\", \"Arnold Schwarzenegger\", \"Megawati Sukarnoputri\", \\\n",
    "          \"Hugo Chavez\", \"Serena Williams\", \"Colin Powell\", \"Junichiro Koizumi\", \"Jennifer Capriati\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecddecb-ec35-4ee5-842e-8e00d8fb1df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of People with Images Available = 5749\n",
      "Top 25 People With Most Images, # of Images\n",
      "('George_W_Bush', 530)\n",
      "('Colin_Powell', 236)\n",
      "('Tony_Blair', 144)\n",
      "('Donald_Rumsfeld', 121)\n",
      "('Gerhard_Schroeder', 109)\n",
      "('Ariel_Sharon', 77)\n",
      "('Hugo_Chavez', 71)\n",
      "('Junichiro_Koizumi', 60)\n",
      "('Jean_Chretien', 55)\n",
      "('John_Ashcroft', 53)\n",
      "('Serena_Williams', 52)\n",
      "('Jacques_Chirac', 52)\n",
      "('Vladimir_Putin', 49)\n",
      "('Luiz_Inacio_Lula_da_Silva', 48)\n",
      "('Gloria_Macapagal_Arroyo', 44)\n",
      "('Arnold_Schwarzenegger', 42)\n",
      "('Jennifer_Capriati', 42)\n",
      "('Laura_Bush', 41)\n",
      "('Lleyton_Hewitt', 41)\n",
      "('Hans_Blix', 39)\n",
      "('Alejandro_Toledo', 39)\n",
      "('Nestor_Kirchner', 37)\n",
      "('Andre_Agassi', 36)\n",
      "('Alvaro_Uribe', 35)\n",
      "('Megawati_Sukarnoputri', 33)\n"
     ]
    }
   ],
   "source": [
    "num_people_with_most_img = 25\n",
    "people_folder = os.listdir(dir_root)\n",
    "num_images_per_folder = [len(os.listdir(os.path.join(dir_root, p))) for p in people_folder]\n",
    "# Get list of tuples containing (Person Name, # of Images), sorted by # of Images in descending order\n",
    "sort_people_num_img = sorted(zip(people_folder, num_images_per_folder), reverse=True, key=lambda pair: pair[1])\n",
    "print(f\"# of People with Images Available = {len(people_folder)}\")\n",
    "print(f\"Top {num_people_with_most_img} People With Most Images, # of Images\")\n",
    "print(*sort_people_num_img[:num_people_with_most_img], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabf2c55-feff-4cba-a4e9-aa9f68ad2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for object detection, facial recognition, and visualizing results\n",
    "def create_training_data(dir_root, people, person_img_max, haar_path, haar_params, detect_objects=False, random_seed=0, verbose=False):\n",
    "    \"\"\"Create training data from the following inputs:\n",
    "    dir_root = a string containing base directory with subfolders containing images for each person\n",
    "    people = a list containing string values, with each string containing a person's name. These are the folder names, EXCEPT for spaces instead of underscores\n",
    "    person_img_max = an integer specifying the maximum number of training images to return for each person. Set this to a value larger than that\n",
    "                     maximum # of images available if you want to process all images\n",
    "    haar_path = a string containing the path to the haar cascade class definition xml file. Files for different features (e.g. face, eyes, etc) can\n",
    "                be downloaded from the following link: https://github.com/opencv/opencv/blob/master/data/haarcascades/\n",
    "    haar_params = a dictionary containing the parameters to pass to the detectMultiScale method of the haar cascade class, e.g. scaleFactor, minNeighbors\n",
    "    detect_objects = a boolean-like value (True/False, 1/0, etc) that turns on/off object detection. Set to false to conduct a dry run that checks\n",
    "                     how many image files will be processed for each person\n",
    "    random_seed = an integer that determines the order of the random shuffle applied to images before selecting the training samples\n",
    "    verbose = a boolean-like value that, when truthy, prints additional details during execution for validation/debugging purposes\n",
    "    Output is a tuple with two values:\n",
    "    features = a list containing zero or more lists, with each list containing four values (x, y, w, h) that define the rectangle containing the detected object\n",
    "    labels = a list containing zero or more integer values, with each int specifying the index to a specific person in the input list of people used for training\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for person in people:\n",
    "        label = people.index(person)\n",
    "        path = os.path.join(dir_root, \"_\".join(person.split(\" \")))\n",
    "        img_files = os.listdir(path)\n",
    "        random.shuffle(img_files)\n",
    "        img_num = 0\n",
    "        total_detected = 0\n",
    "        for img in img_files[:min(person_img_max, len(img_files))]:\n",
    "            img_path = os.path.join(path, img)\n",
    "            img_num += 1\n",
    "            print(f\"Working on Image # {img_num}: {img}\")\n",
    "            if detect_objects:\n",
    "                detected_feature, detected_label = detect_primary_objects(img_path, haar_path, label, verbose=verbose, **haar_params)\n",
    "                if detected_label >= 0:\n",
    "                    total_detected += 1\n",
    "                    features.append(detected_feature)\n",
    "                    labels.append(detected_label)\n",
    "        print(f\"{img_num} training images with {total_detected} objects identified for {person}\")\n",
    "    return features, labels\n",
    "\n",
    "def detect_primary_objects(gray, haar_path, max_iter=10, boost_flag=True, verbose=False, **haar_params):\n",
    "    \"\"\"Identify the \"primary\", or least likely to be a false positive, object detected within the input grayscale image, gray.\n",
    "    The type of object detected by haar_cascade is determined by the haar cascade class xml file that was provided in \n",
    "    the haar_path parameter input to create_training_data.\n",
    "    \"\"\"\n",
    "    haar_cascade = cv.CascadeClassifier(haar_path)\n",
    "    detected_objects = haar_cascade.detectMultiScale(gray, **haar_params)\n",
    "    num_detected = len(detected_objects)\n",
    "    num_detected_prev = num_detected\n",
    "    if verbose:\n",
    "        print(f\"Initial # of Objects Detected = {num_detected}\")\n",
    "    num_iter = 0\n",
    "    boost_factor = 1\n",
    "    while num_detected != 1 and num_iter != max_iter:\n",
    "        num_iter += 1\n",
    "        if verbose:\n",
    "            print(f\"Iteration # = {num_iter}\")\n",
    "        # Update minNeighbors value in copy of params dict\n",
    "        if num_iter == 1:\n",
    "            haar_params_new = haar_params.copy()\n",
    "        elif num_iter == max_iter:\n",
    "            print(f\"Maximum # of iterations ({max_iter}) reached!\")\n",
    "        if num_detected < 1 and haar_params_new['minNeighbors'] > 1:\n",
    "            haar_params_new['minNeighbors'] -= boost_factor\n",
    "        elif num_detected > 1:\n",
    "            haar_params_new['minNeighbors'] += 2 * boost_factor\n",
    "        else:\n",
    "            print(\"Unable to detect object in input image\")\n",
    "            print(f\"Verify that either 1) num_detected is zero ({num_detected==0}) and minNeighbors is one ({haar_params_new['minNeighbors']==1})\")\n",
    "            print(f\"OR 2) the maximum # of iterations has been reached ({num_iter==max_iter})\")\n",
    "            print(\"If either of these scenarios occurs, consider changing the input scaleFactor and/or initial minNeighbors value. If neither 1) or 2) applies, then there is an unknown bug somewhere that should be investigated!!!\")\n",
    "        if verbose:\n",
    "            print(f\"minNeighbors = {haar_params_new['minNeighbors']}\")\n",
    "        detected_objects = haar_cascade.detectMultiScale(gray, **haar_params_new)\n",
    "        num_detected = len(detected_objects)\n",
    "        if num_detected == num_detected_prev and boost_flag:\n",
    "            boost_factor += 1\n",
    "        else:\n",
    "            boost_factor = 1\n",
    "        num_detected_prev = num_detected\n",
    "    if verbose:\n",
    "        print(f\"Final # of Objects Detected = {num_detected}\")\n",
    "    return detected_objects\n",
    "\n",
    "def detect_image_objects(img_path, haar_path, label=-1, verbose=False, **haar_params):\n",
    "    \"\"\"Detect object(s) in the image located at img_path, using the haar object defined in\n",
    "    the xml file located at haar_path where\n",
    "    img_path = a string defining the path to the image file for object detection\n",
    "    haar_path = a string defining the path to the haar cascade class definition xml file as described in create_training_data\n",
    "    label = an integer specifying the index to a specific person in the people list that is the primary person in the image at img_path\n",
    "            When the default value of -1 is provided, then no label is returned, essentially the default is a non-training mode\n",
    "    verbose = a boolean-like value that, when truthy, prints additional details during execution for validation/debugging purposes\n",
    "    haar_params = a dictionary containing the parameters to pass to the detectMultiScale method of the haar cascade class\n",
    "                  Valid values include scaleFactor (default=1.1), minNeighbors (default=3), and minSize. \n",
    "    \"\"\"\n",
    "    img_array = cv.imread(img_path)\n",
    "    gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)\n",
    "    # Detect \"primary\" object in image\n",
    "    detected_rects = detect_primary_objects(gray, haar_path, verbose=verbose, **haar_params)\n",
    "    # detected_rects returns a list that should only have one value, so only one is considered below\n",
    "    # Exception will alert to any issues, and skip detected object so it is NOT included in features and labels\n",
    "    try:\n",
    "        (x, y, w, h) = detected_rects[0]\n",
    "    except Exception as e:\n",
    "        print(f\"The following error occurred when performing object detection for the image at {img_path}:\")\n",
    "        print(e)\n",
    "        x = None\n",
    "    if verbose:\n",
    "        print(*detected_rects[0], sep=\", \")\n",
    "    if isinstance(x, int):\n",
    "        obj_roi = gray[y:y+h, x:x+w]\n",
    "        obj_label = label\n",
    "    else:\n",
    "        obj_roi = None\n",
    "        obj_label = -1\n",
    "    if obj_label > -1:\n",
    "        return obj_roi, obj_label\n",
    "    else:\n",
    "        return obj_roi\n",
    "\n",
    "def show_frame(frame, frame_title):\n",
    "    cv.imshow(frame_title, frame)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "def show_detected_objects(detected_frame, detected_rect, frame_to_show=None, print_detected=False, rect_color=(255, 255, 255), rect_thickness=2):\n",
    "    \"\"\"Display source image with detected object(s) outlined, and optionally display an image focused around each detected object based on a\n",
    "    different input image, frame_to_show. This functionality allow one to show the outline of the detected objects on the grayscale image used\n",
    "    for detection, and also show the bgr image zoomed in on each detected object.\n",
    "    detected_frame = numpy array of uint8 specifying the image used for detection\n",
    "    detected_rect = a list containing zero or more lists, each specifying a rectangle that bounds a detected object in detected_frame\n",
    "    frame_to_show = an alternate image used to display the image contained within each detected_rect. Input MUST be a numpy array in order to turn this feature on\n",
    "    print_detected = boolean-like flag when truthy prints the x, y, w, and h values specifying the rectangle bounding each detected object\n",
    "    rect_color = tuple with three values specifying the (b, g, r) color value for displaying the detected objects\n",
    "    rect_thickness = integer specifying the thickness of the lines defining the rectangle bounding each detected object\n",
    "    \"\"\"\n",
    "    for i, (x, y, w, h) in enumerate(detected_rect):\n",
    "        if print_detected:\n",
    "            print(f\"Object {i} Location: x={x}, y={y}, w={w}, h={h}\")\n",
    "        detected_frame = cv.rectangle(detected_frame, (x, y), (x+w, y+h), rect_color, thickness=rect_thickness)\n",
    "        if isinstance(frame_to_show, np.ndarray):\n",
    "            show_frame(frame_to_show[y:y+h, x:x+w], \"Objects Detected in Image\")\n",
    "    return detected_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4620061-61e9-4f26-bbfe-43758cf660fa",
   "metadata": {},
   "source": [
    "### Load an Example Image and Convert to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c047bf-d25f-45b4-b98f-78a59e198cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [\"Megawati Sukarnoputri\"] # for testing purposes\n",
    "person = people[0]\n",
    "img_num = \"0017\"\n",
    "#img_num = \"0027\"\n",
    "img_ext = \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdb0360-24ea-414c-b18b-1fb651786693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Example Grayscale Image = (250, 250)\n"
     ]
    }
   ],
   "source": [
    "# Parse parameters to load a single grayscale image\n",
    "per_fname = \"_\".join(person.split(\" \"))\n",
    "img_path = os.path.join(dir_root, per_fname, \"_\".join([per_fname, img_num])+img_ext)\n",
    "img_face = cv.imread(img_path)\n",
    "gray_face = cv.cvtColor(img_face, cv.COLOR_BGR2GRAY)\n",
    "print(f\"Size of Example Grayscale Image = {gray_face.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a657b4-3001-4e6a-b14c-10bb2fe365cb",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03584fff-2ebe-4b0f-bbfb-d74f5521d9ab",
   "metadata": {},
   "source": [
    "The first step is to download Haar Cascades xml file for frontal face detection from the following link:\n",
    "\n",
    "https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "\n",
    "If you want to follow the code below, then you should also download the eye and smile detection xml files from the haarcascades folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba68f3e-ef8b-47e3-a663-7a6e665448e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_cascade_params = {\n",
    "    'face': {\n",
    "        'xml_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_frontalface.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 4,\n",
    "           'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'eye': {\n",
    "        'xml_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_eye.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 5,\n",
    "           'minSize': (20, 20)\n",
    "        }\n",
    "    },\n",
    "    'smile': {\n",
    "        'xml_file': '/home/fdpearce/Documents/Projects/models/haar_cascades/haar_cascade_smile.xml',\n",
    "        'params': {\n",
    "            'scaleFactor': 1.1,\n",
    "            'minNeighbors': 20,\n",
    "           'minSize': (20, 20)\n",
    "        }     \n",
    "    }\n",
    "}\n",
    "haar_image = gray_face.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebad2d2b-4f12-4160-a8c3-a88cca6dfcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial # of Objects Detected = 1\n",
      "Final # of Objects Detected = 1\n"
     ]
    }
   ],
   "source": [
    "# Face detection\n",
    "detection_obj = 'face'\n",
    "haar_obj = haar_cascade_params[detection_obj]\n",
    "detected_face_rects = detect_primary_objects(haar_image, haar_obj['xml_file'], verbose=True, **haar_obj['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc74e30-cbef-40ef-935c-2940ccdb4586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial # of Objects Detected = 2\n",
      "Iteration # = 1\n",
      "minNeighbors = 7\n",
      "Iteration # = 2\n",
      "minNeighbors = 11\n",
      "Iteration # = 3\n",
      "minNeighbors = 17\n",
      "Iteration # = 4\n",
      "minNeighbors = 25\n",
      "Iteration # = 5\n",
      "minNeighbors = 35\n",
      "Iteration # = 6\n",
      "minNeighbors = 47\n",
      "Iteration # = 7\n",
      "minNeighbors = 61\n",
      "Iteration # = 8\n",
      "minNeighbors = 77\n",
      "Iteration # = 9\n",
      "minNeighbors = 76\n",
      "Iteration # = 10\n",
      "Maximum # of iterations (10) reached!\n",
      "minNeighbors = 74\n",
      "Final # of Objects Detected = 1\n"
     ]
    }
   ],
   "source": [
    "# Eye detection\n",
    "detection_obj = 'eye'\n",
    "haar_obj = haar_cascade_params[detection_obj]\n",
    "detected_eye_rects = detect_primary_objects(haar_image, haar_obj['xml_file'], verbose=True, **haar_obj['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87bd89a-1d8d-43b5-ae65-894f28321f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial # of Objects Detected = 2\n",
      "Iteration # = 1\n",
      "minNeighbors = 22\n",
      "Iteration # = 2\n",
      "minNeighbors = 26\n",
      "Iteration # = 3\n",
      "minNeighbors = 32\n",
      "Final # of Objects Detected = 1\n"
     ]
    }
   ],
   "source": [
    "# Smile detection\n",
    "detection_obj = 'smile'\n",
    "haar_obj = haar_cascade_params[detection_obj]\n",
    "detected_smile_rects = detect_primary_objects(haar_image, haar_obj['xml_file'], verbose=True, **haar_obj['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb35bfa-51d4-46d7-af90-c56b39b4370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Faces Found = 1\n",
      "# of Eyes Found = 1\n",
      "# of Smiles Found = 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of Faces Found = {len(detected_face_rects)}\")\n",
    "print(f\"# of Eyes Found = {len(detected_eye_rects)}\")\n",
    "print(f\"# of Smiles Found = {len(detected_smile_rects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d354692f-625d-427f-bcaf-6691f9743651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0 Location: x=95, y=142, w=58, h=29\n"
     ]
    }
   ],
   "source": [
    "# Print detected smile(s) coordinates, and optionally show on original image\n",
    "haar_image = show_detected_objects(haar_image, detected_smile_rects, print_detected=True, frame_to_show=None, rect_color=(255, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084e094c-e03e-4172-ae73-da04c71ef2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0 Location: x=131, y=99, w=30, h=30\n"
     ]
    }
   ],
   "source": [
    "# Print detected eye(s) coordinates, and optionally show on original image\n",
    "haar_image = show_detected_objects(haar_image, detected_eye_rects, print_detected=True, frame_to_show=None, rect_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4f60c6-5f64-4780-9a6d-6349c14b44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0 Location: x=68, y=67, w=119, h=119\n"
     ]
    }
   ],
   "source": [
    "# Print detected face(s) coordinates, and optionally show on original image\n",
    "haar_image = show_detected_objects(haar_image, detected_face_rects, print_detected=True, frame_to_show=None, rect_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45157fce-0d33-436f-9fee-841a7830be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_frame(haar_image, f\"Faces(s) Detected in Image of {person}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df262bd-9468-426d-8a07-ff6a3df22ce8",
   "metadata": {},
   "source": [
    "## Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c0e43-153e-490b-841b-767ce8887ab4",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a059c2-d9c2-497e-a298-6bd45a8de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_max = 20\n",
    "haar_face = \"haar_cascade_frontalface.xml\" # Path to haar cascade xml file\n",
    "face_params = {'scaleFactor': 1.1,\n",
    "               'minNeighbors': 4,\n",
    "               'minSize': (20, 20)}\n",
    "features, labels = create_training_data(dir_root, people, person_img_max, haar_face, face_params, detect_objects=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d1a4b-7dc4-44d6-970d-b4a51893a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of Features = {len(features)}\")\n",
    "print(f\"# of Labels = {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad592d-227f-432f-a312-b793610f87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect example images and the bounding box detected by the face recognition algorithm\n",
    "# Use the verbose setting above to print out bounding boxes\n",
    "person = people[0]\n",
    "img_num = [\"0017\", \"0012\", \"0014\", \"0029\", \"0013\", \"0027\", \"0020\"]\n",
    "img_ext = \".jpg\"\n",
    "face_rectangles = [[[72, 69, 113, 113]], \\\n",
    "              [[73, 73, 110, 110]], \\\n",
    "              [[61, 61, 126, 126]], \\\n",
    "              [[64, 63, 123, 123]], \\\n",
    "              [[68, 65, 119, 119]], \\\n",
    "              [[70, 65, 118, 118]], \\\n",
    "              [[70, 69, 113, 113]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29f379-c4fe-42c2-9dc2-cb625c5d9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, im_num in enumerate(img_num):\n",
    "    per_fname = \"_\".join(person.split(\" \"))\n",
    "    img_path = os.path.join(dir_root, per_fname, \"_\".join([per_fname, im_num])+img_ext)\n",
    "    img_face = cv.imread(img_path)\n",
    "    gray_face = cv.cvtColor(img_face, cv.COLOR_BGR2GRAY)\n",
    "    gray_face = show_detected_objects(gray_face, face_rectangles[ind], print_detected=True, frame_to_show=None, rect_color=(0, 0, 0))\n",
    "    show_frame(gray_face, f\"Primary Face Detected for {person}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1816058-18ce-4bf9-a343-5134746fa7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb676fb2-17db-4c08-8a8e-aa19febae28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a32bf7-6694-4455-9679-5e56cea19398",
   "metadata": {},
   "source": [
    "### 2. Train Face Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa0419-ab32-46ef-83ac-ccd1f325e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_recognizer_data = False\n",
    "face_recognizer = cv.face.LBPHFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954fac-95cf-4d7f-8220-bead53aea7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features, dtype=\"object\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fc96f-d9fc-4dd4-be7e-b881a835b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognizer.train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a1b1d-adb5-4d54-a99d-c2871fca2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_recognizer_data:\n",
    "    face_recognizer.save(\"face_recognizer.yml\")\n",
    "    np.save(\"features.npy\", features)\n",
    "    np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87352c-4698-45d7-90e7-28e44866b6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167eb950-a8d4-47da-9601-cb29489a5b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
